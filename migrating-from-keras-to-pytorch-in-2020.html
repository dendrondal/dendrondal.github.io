<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns#">
<head>
  <meta charset="UTF-8" />
  <title>Migrating from Keras to PyTorch in 2020 | Dal&#39;s Portfolio</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400i,600,600i%7CSource+Code+Pro:400,400i,600" />
  <link rel="stylesheet" href="/static/m-dark.css" />
  <link rel="icon" href="/images/favicon.ico" type="image/x-ico" />
  <link rel="canonical" href="/migrating-from-keras-to-pytorch-in-2020.html" />
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Dal&#39;s Portfolio" />
  <link href="/feeds/tutorials.atom.xml" type="application/atom+xml" rel="alternate" title="Dal&#39;s Portfolio | Tutorials" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="theme-color" content="#353535" />
  <meta name="twitter:site" content="@dendrondal" />
  <meta property="og:site_name" content="Dal&#39;s Portfolio" />
  <meta property="og:title" content="Migrating from Keras to PyTorch in 2020" />
  <meta name="twitter:title" content="Migrating from Keras to PyTorch in 2020" />
  <meta property="og:url" content="/migrating-from-keras-to-pytorch-in-2020.html" />
  <meta property="og:description" content="Note: this specifically applies to PyTorch 1.5 and tf.Keras 2.1.0. As long as the final number in the version is the same, this should still be applicable, but otherwise, YMMV While completing my fellowship at Insight, I reached a point in my recyclable classification project where …" />
  <meta name="twitter:description" content="Note: this specifically applies to PyTorch 1.5 and tf.Keras 2.1.0. As long as the final number in the version is the same, this should still be applicable, but otherwise, YMMV While completing my fellowship at Insight, I reached a point in my recyclable classification project where …" />
  <meta name="twitter:card" content="summary" />
  <meta property="og:type" content="article" />
</head>
<body>
<header><nav id="navigation">
  <div class="m-container">
    <div class="m-row">
      <a href="/" id="m-navbar-brand" class="m-col-t-9 m-col-m-none m-left-m"><img src="/images/tree.jpg" alt="" />Dal&#39;s Portfolio</a>
      <a id="m-navbar-show" href="#navigation" title="Show navigation" class="m-col-t-3 m-hide-m m-text-right"></a>
      <a id="m-navbar-hide" href="#" title="Hide navigation" class="m-col-t-3 m-hide-m m-text-right"></a>
      <div id="m-navbar-collapse" class="m-col-t-12 m-show-m m-col-m-none m-right-m">
        <div class="m-row">
          <ol class="m-col-t-12 m-col-m-none">
            <li><a href="/pages/about-me.html">About</a></li>
            <li><a href="https://github.com/dendrondal">GitHub</a></li>
            <li><a href="https://linkedin.com/in/dal-williams">LinkedIn</a></li>
            <li><a href="https://twitter.com/dendrondal">Twitter</a></li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</nav></header>
<main>
<div class="m-container">
  <div class="m-row">
    <article class="m-col-m-10 m-nopadb">
      <header>
        <h1><a href="/migrating-from-keras-to-pytorch-in-2020.html" rel="bookmark" title="Permalink to Migrating from Keras to PyTorch in 2020">
          <time class="m-date" datetime="2020-05-05T00:00:00-05:00">
            May <span class="m-date-day">05</span> 2020
          </time>
          Migrating from Keras to PyTorch in 2020
        </a></h1>
        <p><em>Note: this specifically applies to PyTorch 1.5 and tf.Keras 2.1.0. As
        long as the final number in the version is the same, this should still
        be applicable, but otherwise, YMMV</em></p>
        <p>While completing my fellowship at <a href="insightdatascience.com">Insight</a>,
        I reached a point in my <a href="github.com/dendrondal/CIf3R">recyclable classification
        project</a> where …</p>
      </header>
      <div class="m-clearfix-l"></div>
<!-- content -->
<p><em>Note: this specifically applies to PyTorch 1.5 and tf.Keras 2.1.0. As
long as the final number in the version is the same, this should still
be applicable, but otherwise, YMMV</em></p>
<p>While completing my fellowship at <a href="insightdatascience.com">Insight</a>,
I reached a point in my <a href="github.com/dendrondal/CIf3R">recyclable classification
project</a> where I wanted to implement a
<a href="https://sorenbouma.github.io/blog/oneshot/">siamese network</a> to improve the generalizability of my model,
while requiring less training data. The ad-hoc siamese networks I
created in Keras had acceptable results, but were certainly not
production-ready. I began looking on <a href="paperswithcode.com">papers with code</a> for a more
SOTA model, and realized that Keras implementations are few and far
between. In fact, PyTorch seems to have <a href="https://paperswithcode.com/trends">exploded in popularity</a> to
the point where it is now the #1 framework on papers with code. With
this in mind, I decided it was time to start learning PyTorch.</p>
<section id="why-move">
<h2>Why move?</h2>
<p>In addition to the predominance of PyTorch on paperswithcode, there are
several advantages it offers. Back when I began learning Keras,
TensorFlow was the dominant framework partially due to easier
productionalization of models using tf.Serve. However, since then, the
productionalization gap <a href="https://engineering.fb.com/ai-research/announcing-pytorch-1-0-for-both-research-and-production/">appears to be
closing</a>.
In addition, PyTorch comes with out-of-the-box asynchronous behavior
when loading, whereas this still has to be specified and configured when
using tf.Datasets. TensorBoard was, and still is, the predominant
monitoring solution for model training, but it’s now fully supported by
PyTorch. Finally, in my opinion, TensorFlow has made some rather strange
api decisions, whereas PyTorch seems to bear more similarity to Numpy.</p>
</section>
<section id="defining-the-computation-graph">
<h2>Defining the computation graph</h2>
<p>Since Keras has both the sequential and functional api, it’s worth doing
a comparison of both with the preferred way of instantiating a PyTorch
network. Here, let’s use a <a href="https://www.google.com/books/edition/Deep_Learning_with_Python/Yo3CAQAACAAJ?hl=en">simple feedforward
MLP</a>
as an example:</p>
<pre class="m-code"><span class="c1">#Keras sequential api</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>

<span class="c1">#Keras functional api</span>
<span class="n">input_layer</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">input_layer</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">input_layer</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

<span class="c1">#Pytorch example</span>
<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span></pre>
<p>The obvious difference here is that the standard practice for PyTorch
models is encapsulation into a class, and instantiating the parent
<code>Module</code> class. Another less obvious difference is that both the input
and output sizes of each layer have to be specified in PyTorch. Other
than that, it’s a pretty straightforward difference. Let’s try a less
trivial example that’s more suited to the Keras functional API: a
siamese network for image similarity classification, based on the
<a href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf">original paper</a> with several convolutional layers taken out for
the sake of length. This employs custom layers, shared weights, and
gives an overview of how a multi-input, single output neural network can
be implemented in the two frameworks.</p>
<pre class="m-code"><span class="c1">#Keras example</span>
<span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">105</span><span class="p">,</span> <span class="mi">105</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">img_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">)</span>
<span class="n">left_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">)</span>
<span class="n">right_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">)(</span><span class="n">img_input</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">2048</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">twin</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">img_input</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>

<span class="c1"># get the image embeddings, sharing weights between the two networks</span>
<span class="n">encoded_l</span> <span class="o">=</span> <span class="n">twin</span><span class="p">(</span><span class="n">left_input</span><span class="p">)</span>
<span class="n">encoded_r</span> <span class="o">=</span> <span class="n">twin</span><span class="p">(</span><span class="n">right_input</span><span class="p">)</span>

<span class="c1"># merge two encoded inputs with the l1 distance between them</span>
<span class="n">L1_layer</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">L1_distance</span> <span class="o">=</span> <span class="n">L1_layer</span><span class="p">([</span><span class="n">encoded_l</span><span class="p">,</span> <span class="n">encoded_r</span><span class="p">])</span>

<span class="n">prediction</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)(</span><span class="n">L1_distance</span><span class="p">)</span>

<span class="n">siamese_net</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">left_input</span><span class="p">,</span> <span class="n">right_input</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">prediction</span><span class="p">)</span>

<span class="c1"># The same model in PyTorch    correct = 0</span>
<span class="k">class</span> <span class="nc">SiameseNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2048</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span></pre>
<p>Unlike Keras, convolutional layers in PyTorch have arguments in the
order of
<code>in_channel size, out_channels size, kernel_size, stride, padding</code>,
with the default stride and padding being 1 and 0, respectively. You’re
probably noticing that with the PyTorch model, we stopped around the
<code>twin</code> definition in the Keras model. The reason being is that the
biggest difference between keras and pytorch is how you train the model,
aka the training loop.</p>
</section>
<section id="training-the-model">
<h2>Training the model</h2>
<p>Defining the model isn’t very different between Keras and PyTorch, but
training the model certainly is. Rather than calling
<code>model.compile()</code>, you instead define your forward pass as a method of
your model. Also, your loss function, optimizer, and learning rate are
usually defined in the training loop. Let’s start with the forward pass
and training loop for our first MLP:</p>
<pre class="m-code"><span class="c1"># Defining the forward pass. Note that this is a method of Model</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># F is an alias for torch.nn.functional</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span></pre>
<p>This shows two methods of model creation: for the siamese model, we
define the entire model intially, making the forward pass as simple as
<code>return twin(x)</code>. With the MLP, we defined the layers individually.
Which method is better definitely depends on your use case, but my
intuition is that a neural network that can be drawn as a linear
progression of layers lends itself well to the MLP method, whereas
defining your entire model as an attribute works well for more advanced
graphs such as ResNet/Inception type models, or models with multimodal
input/outputs. Alright, so we have our model and how our data flows
through it. The next step is training and evaluation. This is indeed far
more code than is needed by using <code>callbacks</code> in Keras, but the
training loop adds complexity in exchange for significantly more
flexibility.</p>
<pre class="m-code"><span class="c1"># train_loader is some predefined Torch DataLoader instance</span>
<span class="c1"># device is your cpu/gpu name</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="c1"># Assuming X and y are torch tensors, you can also just call X.cuda() instead if</span>
        <span class="c1"># you know you don&#39;t need to switch devices.</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># We re-instantiate the gradients during each iteration</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="c1"># Now we back-propagate</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="c1"># Report accuracy every 10 batches</span>
        <span class="k">if</span> <span class="n">batch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Loss of </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s1"> after </span><span class="si">{</span><span class="n">batch</span><span class="si">}</span><span class="s1"> epochs on training set&#39;</span><span class="p">)</span></pre>
<p>The function above is meant to be used in a <code>for</code> loop with a preset
number of epochs. Optimizers are called in a similar manner compared to
Keras. Similar to the LearningRateScheduler in Keras’ callbacks, we now
have several built-in <a href="https://pytorch.org/docs/stable/optim.html?highlight=scheduler#torch.optim.lr_scheduler.StepLR">adaptive learning
rates</a>.
We have our training function, now for the test one:</p>
<pre class="m-code"><span class="c1"># Again, test_loader is a DataLoader instance</span>
<span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">test_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># We don&#39;t want to change the gradients, so we freeze the model here</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">y_hat</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># For binary classification</span>
            <span class="c1"># For multiclass, pass keepdim=True above</span>
            <span class="c1"># Now we format the actual target and compare it to the predicted one</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="n">pred</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">pred</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">test_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Average loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">}</span><span class="se">\n</span><span class="s1">Accuracy: </span><span class="si">{</span><span class="n">correct</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span></pre>
<p>Now we have our model with its foward propagation method, a training
function, and a testing function. We presume there is a data loading
function in there somewhere as well. So the final step is putting it all
together, either in script for or in a <code>main</code> function for CLI
execution. Here is the last bit in script form:</p>
<pre class="m-code"><span class="c1"># With Torch, we have to specify GPU/CPU computation</span>
<span class="n">use_cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">use_cuda</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="c1"># First we load the model onto the GPU</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># Now we load our optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="c1"># Let&#39;s also apply a learning rate decay</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
<span class="c1"># Now let&#39;s train for 100 epochs</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
   <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
   <span class="n">test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
   <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="c1">#Saving the weights of the model to a pickle file</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;torch_example.pt&#39;</span><span class="p">)</span></pre>
<p>Whew, that’s a lot of code for a 3 layer MLP! Of course, this is only a
starting point. You’ll probably want some kind of early stopping
mechanism, monitoring with tensorboard or custom visualizations, a tqdm
progress bar, and/or logging. In performing this excercise, I’m of the
mind that the additional code is actually a good thing, as debugging
becomes far easier as you can isolate the line causing the issue with a
visual debugger (<em>cough</em> or a print statement <em>cough</em>), as opposed to
Keras abstracting that complexity away.</p>
<p>So this post doesn’t get too long, I’m going to direct you to <a href="https://github.com/dendrondal/CIf3R">the
repository for my Insight project</a> if you want to see the siamese
network in PyTorch. Overall, PyTorch is pretty great, and a smoother
transition than I originally thought. I’ll have to see if this is just a
honeymoon phase, but I figure there’s likely a reason there are so many
converts as of late. Happy hacking, and thanks for reading!</p>
</section>
<!-- /content -->
      <footer>
        <p>Posted by <a href="/author/jon-steven-dal-williams.html">Jon Steven Dal Williams</a> on <time datetime="2020-05-05T00:00:00-05:00">Tue 05 May 2020</time> in <a href="/category/tutorials.html">Tutorials</a>. Tags: <a href="/tag/keras.html">Keras</a>, <a href="/tag/pytorch.html">PyTorch</a>, <a href="/tag/tensorflow.html">TensorFlow</a>, <a href="/tag/cnns.html">CNNs</a>, <a href="/tag/computer-vision.html">computer vision</a>, <a href="/tag/tutorials.html">tutorials</a>.</p>
      </footer>
    </article>
    <nav class="m-navpanel m-col-m-2">
      <h3>Categories</h3>
      <ol class="m-block-bar-m">
        <li><a href="/category/blog.html">Blog</a></li>
        <li><a href="/category/projects.html">Projects</a></li>
        <li><a href="/category/talks.html">Talks</a></li>
        <li><a href="/category/tutorials.html">Tutorials</a></li>
      </ol>
      <h3>Tag cloud</h3>
      <ul class="m-tagcloud">
        <li class="m-tag-3"><a href="/tag/airflow.html">Airflow</a></li>
        <li class="m-tag-3"><a href="/tag/altair.html">Altair</a></li>
        <li class="m-tag-3"><a href="/tag/aws.html">AWS</a></li>
        <li class="m-tag-3"><a href="/tag/bash.html">Bash</a></li>
        <li class="m-tag-3"><a href="/tag/bayesian-stats.html">Bayesian stats</a></li>
        <li class="m-tag-3"><a href="/tag/chatbots.html">chatbots</a></li>
        <li class="m-tag-3"><a href="/tag/chemistry.html">chemistry</a></li>
        <li class="m-tag-3"><a href="/tag/cnns.html">CNNs</a></li>
        <li class="m-tag-3"><a href="/tag/computer-vision.html">computer vision</a></li>
        <li class="m-tag-5"><a href="/tag/flask.html">Flask</a></li>
        <li class="m-tag-3"><a href="/tag/functional-programming.html">functional programming</a></li>
        <li class="m-tag-3"><a href="/tag/gans.html">GANs</a></li>
        <li class="m-tag-3"><a href="/tag/jupyter.html">Jupyter</a></li>
        <li class="m-tag-3"><a href="/tag/keras.html">Keras</a></li>
        <li class="m-tag-3"><a href="/tag/latex.html">LaTeX</a></li>
        <li class="m-tag-3"><a href="/tag/luigi.html">Luigi</a></li>
        <li class="m-tag-3"><a href="/tag/make.html">Make</a></li>
        <li class="m-tag-3"><a href="/tag/markdown.html">Markdown</a></li>
        <li class="m-tag-3"><a href="/tag/microsoft.html">Microsoft</a></li>
        <li class="m-tag-5"><a href="/tag/mission-driven-data-science.html">Mission-driven data science</a></li>
        <li class="m-tag-3"><a href="/tag/mongodb.html">MongoDB</a></li>
        <li class="m-tag-3"><a href="/tag/pandas.html">Pandas</a></li>
        <li class="m-tag-3"><a href="/tag/papermill.html">Papermill</a></li>
        <li class="m-tag-3"><a href="/tag/pytorch.html">PyTorch</a></li>
        <li class="m-tag-3"><a href="/tag/reproducibility.html">reproducibility</a></li>
        <li class="m-tag-3"><a href="/tag/rest.html">REST</a></li>
        <li class="m-tag-3"><a href="/tag/science-writing.html">Science Writing</a></li>
        <li class="m-tag-3"><a href="/tag/scikit-learn.html">SciKit Learn</a></li>
        <li class="m-tag-3"><a href="/tag/spacy.html">Spacy</a></li>
        <li class="m-tag-3"><a href="/tag/sqalchemy.html">SQAlchemy</a></li>
        <li class="m-tag-3"><a href="/tag/sqlite.html">SQLite</a></li>
        <li class="m-tag-3"><a href="/tag/swagger.html">Swagger</a></li>
        <li class="m-tag-3"><a href="/tag/tensorflow.html">TensorFlow</a></li>
        <li class="m-tag-5"><a href="/tag/tutorials.html">tutorials</a></li>
        <li class="m-tag-3"><a href="/tag/vaes.html">VAEs</a></li>
        <li class="m-tag-3"><a href="/tag/wild-python.html">wild python</a></li>
      </ul>
    </nav>
  </div>
</div>
</main>
<footer><nav>
  <div class="m-container">
    <div class="m-row">
      <div class="m-col-l-10 m-push-l-1">
        <p>Dal's Portfolio. Powered by <a href="https://getpelican.com">Pelican</a> and <a href="https://mcss.mosra.cz">m.css</a>.</p>
      </div>
    </div>
  </div>
</nav></footer>
</body>
</html>